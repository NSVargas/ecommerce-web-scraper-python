import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# ==========================================
# CONFIGURACI√ìN
# ==========================================
BASE_URL = 'http://books.toscrape.com/catalogue/page-{}.html'
DATA_EXPORT_FILE = 'libros_extraidos.csv'

def obtener_datos_pagina(numero_pagina):
    """
    Funci√≥n que descarga y parsea una p√°gina espec√≠fica.
    """
    url = BASE_URL.format(numero_pagina)
    print(f"üîÑ Scrapeando p√°gina: {numero_pagina}...")
    
    try:
        # Simulamos un User-Agent real para evitar bloqueos b√°sicos
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            libros = soup.find_all('article', class_='product_pod')
            
            datos_libros = []
            
            for libro in libros:
                # Extracci√≥n de datos con manejo de errores por si falta alg√∫n campo
                try:
                    titulo = libro.h3.a['title']
                    precio = libro.find('p', class_='price_color').text
                    # Limpieza b√°sica de datos (quitar s√≠mbolo de moneda)
                    precio_limpio = float(precio.replace('¬£', '').replace('√Ç', ''))
                    disponibilidad = libro.find('p', class_='instock availability').text.strip()
                    rating = libro.find('p', class_='star-rating')['class'][1]
                    
                    datos_libros.append({
                        'Titulo': titulo,
                        'Precio (¬£)': precio_limpio,
                        'Disponibilidad': disponibilidad,
                        'Rating': rating
                    })
                except Exception as e:
                    print(f"‚ö†Ô∏è Error al procesar un libro: {e}")
            
            return datos_libros
        else:
            print(f"‚ùå Error al cargar la p√°gina {numero_pagina}. Status: {response.status_code}")
            return []

    except Exception as e:
        print(f"‚ùå Error de conexi√≥n: {e}")
        return []

def main():
    """
    Funci√≥n principal que orquesta el scraping de m√∫ltiples p√°ginas.
    """
    print("üöÄ Iniciando Scraper de Libros...")
    todos_los_libros = []
    
    # Scrapeamos las primeras 3 p√°ginas como demo
    for i in range(1, 4): 
        libros_pagina = obtener_datos_pagina(i)
        todos_los_libros.extend(libros_pagina)
        
        # Pausa √©tica para no saturar el servidor (buena pr√°ctica de ingenier√≠a)
        time.sleep(random.uniform(1, 2))
    
    # Exportar a Excel/CSV con Pandas
    if todos_los_libros:
        df = pd.DataFrame(todos_los_libros)
        df.to_csv(DATA_EXPORT_FILE, index=False, encoding='utf-8-sig')
        print(f"\n‚úÖ √âXITO: Se han extra√≠do {len(df)} libros.")
        print(f"üìÅ Datos guardados en: {DATA_EXPORT_FILE}")
        
        # Mostrar una vista previa
        print("\nVista previa de los datos:")
        print(df.head())
    else:
        print("\n‚ö†Ô∏è No se encontraron datos.")

if __name__ == "__main__":
    main()